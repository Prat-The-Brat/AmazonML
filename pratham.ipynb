{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyobjc-core in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (10.3.1)\n",
      "Requirement already satisfied: pyobjc-framework-Quartz in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (10.3.1)\n",
      "Requirement already satisfied: pyobjc-framework-Vision in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (10.3.1)\n",
      "Requirement already satisfied: wurlitzer in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (3.1.1)\n",
      "Requirement already satisfied: pyobjc-framework-Cocoa>=10.3.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from pyobjc-framework-Quartz) (10.3.1)\n",
      "Requirement already satisfied: pyobjc-framework-CoreML>=10.3.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from pyobjc-framework-Vision) (10.3.1)\n",
      "Results saved to output_text.csv\n"
     ]
    }
   ],
   "source": [
    "!pip install pyobjc-core pyobjc-framework-Quartz pyobjc-framework-Vision wurlitzer\n",
    "import csv\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import Quartz\n",
    "import Vision\n",
    "from Cocoa import NSURL\n",
    "from Foundation import NSDictionary\n",
    "from wurlitzer import pipes\n",
    "\n",
    "def image_to_text(img_path, lang=\"eng\"):\n",
    "    input_url = NSURL.fileURLWithPath_(img_path)\n",
    "\n",
    "    with pipes() as (out, err):\n",
    "        input_image = Quartz.CIImage.imageWithContentsOfURL_(input_url)\n",
    "\n",
    "    vision_options = NSDictionary.dictionaryWithDictionary_({})\n",
    "    vision_handler = Vision.VNImageRequestHandler.alloc().initWithCIImage_options_(\n",
    "        input_image, vision_options\n",
    "    )\n",
    "    results = []\n",
    "    handler = make_request_handler(results)\n",
    "    vision_request = Vision.VNRecognizeTextRequest.alloc().initWithCompletionHandler_(handler)\n",
    "    error = vision_handler.performRequests_error_([vision_request], None)\n",
    "\n",
    "    return results\n",
    "\n",
    "def make_request_handler(results):\n",
    "    \"\"\" results: list to store results \"\"\"\n",
    "    if not isinstance(results, list):\n",
    "        raise ValueError(\"results must be a list\")\n",
    "\n",
    "    def handler(request, error):\n",
    "        if error:\n",
    "            print(f\"Error! {error}\")\n",
    "        else:\n",
    "            observations = request.results()\n",
    "            for text_observation in observations:\n",
    "                recognized_text = text_observation.topCandidates_(1)[0]\n",
    "                results.append([recognized_text.string(), recognized_text.confidence()])\n",
    "    return handler\n",
    "\n",
    "def download_image(image_url):\n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        img_path = 'temp_image.jpg'\n",
    "        img.save(img_path)\n",
    "        return img_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {image_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_images_and_save_to_csv(csv_file):\n",
    "    with open(csv_file, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header\n",
    "        data = list(reader)[:100]  # Get first 100 rows\n",
    "\n",
    "    output_data = []\n",
    "    for row in data:\n",
    "        image_url, group_id, entity_name, entity_value = row\n",
    "        img_path = download_image(image_url)\n",
    "\n",
    "        if img_path:\n",
    "            text_results = image_to_text(img_path)\n",
    "            if text_results:\n",
    "                extracted_text = text_results[0][0]  # Take first recognized text\n",
    "                output_data.append([image_url, group_id, entity_name, entity_value, extracted_text])\n",
    "            else:\n",
    "                output_data.append([image_url, group_id, entity_name, entity_value, \"No text found\"])\n",
    "\n",
    "    # Save results to a new CSV\n",
    "    output_csv = \"output_text.csv\"\n",
    "    with open(output_csv, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['image_link', 'group_id', 'entity_name', 'entity_value', 'extracted_text'])\n",
    "        writer.writerows(output_data)\n",
    "    \n",
    "    print(f\"Results saved to {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify your train dataset CSV file here\n",
    "    csv_file = '/Users/prathambhonge/AmazonML/student_resource 3/dataset/train.csv'\n",
    "    process_images_and_save_to_csv(csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize the text and label tokens that belong to an entity and create databin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m             train_data\u001b[38;5;241m.\u001b[39mappend((text, annotations))\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_data\n\u001b[0;32m---> 32\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextracted_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_link\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: extract_text_from_image(x))  \u001b[38;5;66;03m# Extract text using Tesseract\u001b[39;00m\n\u001b[1;32m     34\u001b[0m train_data \u001b[38;5;241m=\u001b[39m create_training_data(train_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def label_entities(text, entity_name, entity_value):\n",
    "    \"\"\"This function will label the tokens in the text as entity or not\"\"\"\n",
    "    words = text.split() \n",
    "    entities = []\n",
    "    \n",
    "    for word in words:\n",
    "        if entity_value in word:\n",
    "            entities.append((word, entity_name))\n",
    "        else:\n",
    "            entities.append((word, \"O\"))  # \"O\" means outside any entity\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def create_training_data(train_df):\n",
    "    train_data = []\n",
    "    \n",
    "    for _, row in train_df.iterrows():\n",
    "        text = preprocess_text(row['extracted_text'])  # Assume 'extracted_text' contains the OCR output\n",
    "        entities = label_entities(text, row['entity_name'], row['entity_value'])\n",
    "        annotations = {\"entities\": []}\n",
    "        \n",
    "        for word, label in entities:\n",
    "            start = text.find(word)\n",
    "            end = start + len(word)\n",
    "            annotations['entities'].append((start, end, label))\n",
    "        \n",
    "        if annotations['entities']:\n",
    "            train_data.append((text, annotations))\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "train_df['extracted_text'] = train_df['image_link'].apply(lambda x: extract_text_from_image(x))  # Extract text using Tesseract\n",
    "train_data = create_training_data(train_df)\n",
    "\n",
    "def create_docbin(train_data):\n",
    "    db = DocBin()\n",
    "    for text, annotations in train_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annotations[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is not None:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    return db\n",
    "\n",
    "train_db = create_docbin(train_data)\n",
    "train_db.to_disk(\"./train.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training import Example\n",
    "import random\n",
    "\n",
    "def train_ner_model(nlp, train_db_path, n_iter=20):\n",
    "    # Load the training data\n",
    "    train_db = DocBin().from_disk(train_db_path)\n",
    "    train_examples = []\n",
    "    \n",
    "    for doc in train_db.get_docs(nlp.vocab):\n",
    "        example = Example.from_dict(doc, {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]})\n",
    "        train_examples.append(example)\n",
    "\n",
    "    # Start the training process\n",
    "    optimizer = nlp.begin_training()\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        random.shuffle(train_examples)\n",
    "        losses = {}\n",
    "        batches = spacy.util.minibatch(train_examples, size=spacy.util.compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            nlp.update(batch, drop=0.5, losses=losses)\n",
    "        print(f\"Iteration {i}: Losses {losses}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    nlp.to_disk(\"ner_model\")\n",
    "\n",
    "# Train the model with 20 iterations\n",
    "train_ner_model(nlp, \"./train.spacy\", n_iter=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entities(nlp, text):\n",
    "    doc = nlp(text)\n",
    "    entities = {}\n",
    "    for ent in doc.ents:\n",
    "        entities[ent.label_] = ent.text\n",
    "    return entities\n",
    "\n",
    "nlp = spacy.load(\"ner_model\")\n",
    "\n",
    "test_text = preprocess_text(extract_text_from_image('test_image_url'))\n",
    "predicted_entities = predict_entities(nlp, test_text)\n",
    "\n",
    "print(predicted_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def format_predictions(test_df, predictions):\n",
    "    output = pd.DataFrame(columns=[\"index\", \"prediction\"])\n",
    "    for idx, row in test_df.iterrows():\n",
    "        prediction = predictions.get(row[\"entity_name\"], \"\")\n",
    "        output = output.append({\"index\": row[\"index\"], \"prediction\": prediction}, ignore_index=True)\n",
    "    return output\n",
    "\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "test_df['extracted_text'] = test_df['image_link'].apply(lambda x: extract_text_from_image(x))  # Extract text using Tesseract\n",
    "test_predictions = [predict_entities(nlp, preprocess_text(text)) for text in test_df['extracted_text']]\n",
    "\n",
    "formatted_output = format_predictions(test_df, test_predictions)\n",
    "formatted_output.to_csv('test_out.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
